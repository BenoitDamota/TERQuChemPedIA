{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recherche d'une bonne fonction de coût pour le modèle DELTA_DIST+H_05\n",
    "\n",
    "On a remarqué que les modèles prédisent systématiquement des valeurs similaires qui minimisent en moyenne la fonction de coût mais qui ne sont pas liées à la géométrie des molécules.\n",
    "\n",
    "Pour parer à ce problème, nous allons définir ici des fonctions de coût alternatives qui auront pour but de pénaliser les modèles s'ils s'approchent de la \"solution de facilité\" qu'est la prédiction systématique d'une valeur minimisant en moyenne la fonction de coût.\n",
    "\n",
    "Pour évaluer les différentes fonctions de coût, nous utiliserons toujours le RMSE partiel actuel comme mesure de validation. Si nous trouvons une fonction qui entraîne un modèle dont les prédictions ont un RMSE partiel inférieur à 107, alors cette fonction de coût sera considérée comme intéressante.\n",
    "\n",
    "\n",
    "#### Chemin des fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prepared_input_loc = \"../data/train_set_riken_v2_prepared_input_bruit+_anums_reduced.h5\"\n",
    "train_labels_loc = \"../data/train_set_riken_v2_labels_bruit+_anums_reduced.h5\"\n",
    "\n",
    "mini_prepared_input_loc = \"../data/mini_set_prepared_input_bruit+_anums_reduced.h5\"\n",
    "mini_labels_loc = \"../data/mini_set_labels_bruit+_anums_reduced.h5\"\n",
    "\n",
    "models_loc = \"../models/DELTA_DIST+H_05/8.5/\"\n",
    "logs_loc = \"../models/DELTA_DIST+H_05/8.5/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition des fonctions de coût\n",
    "\n",
    "#### Calcul du masque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/etudiant/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def calcul_masque_atomes_definis(targets):\n",
    "    \"\"\" On calcule le masque booléen des atomes donnés en entrée du RN en fonction du vecteur targets\"\"\"\n",
    "    \n",
    "    # On cherche à obtenir un masque booléen des atomes définis en entrée. Pour cela, on prend en entrée\n",
    "    # les étiquettes sous la forme d'une matrice (200, 4) dont chaque ligne i est la distance de l'atome i avec\n",
    "    # les atomes fictifs du repère. L'atome est indéfini ssi. la somme de la ligne est nulle. En effet,\n",
    "    # un atome défini ne peut pas avoir une distance nulle avec les quatre atomes fictifs, et on veille\n",
    "    # à ce que le vecteurs targets ne contienne que des valeurs nulles pour les atomes non définis.\n",
    "    # On obtient donc un masque booléen de tous les atomes définis en entrée\n",
    "    \n",
    "    ## On somme les distances de chaque atome ##\n",
    "    targets_dists_sums = tf.reduce_sum(targets, 1)\n",
    "    \n",
    "    ## On créé le masque des sommes différentes de zéro ##\n",
    "    \n",
    "    # Création des matrice de True et de False de la dimension de la matrice des sommes (nécessaires\n",
    "    # pour tf.where)\n",
    "    zeros = tf.cast(tf.zeros_like(targets_dists_sums),dtype=tf.bool)\n",
    "    ones = tf.cast(tf.ones_like(targets_dists_sums),dtype=tf.bool)\n",
    "    \n",
    "    return tf.where(targets_dists_sums>0, ones, zeros)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calcul du RMSE partiel en tant que fonction de validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_rmse(predictions, targets, inputs):\n",
    "    \"\"\" Calcule le RMSE partiel des prédictions par rapport aux valeurs attendues. Le RMSE est partiel car\n",
    "    on ne le calcule que pour les sorties correspondant aux atomes donnés en entrée. En d'autres\n",
    "    termes, on ne pousse pas le modèle à donner des distances nulles pour les atomes indéfinis\n",
    "    en entrée\"\"\"\n",
    "    \n",
    "    with tf.name_scope(\"partial_rmse_validation\"):\n",
    "\n",
    "        # On met les prédictions et les cibles sous la forme d'une matrice (200, 4)\n",
    "        predictions = tf.reshape(predictions, [-1, 4])\n",
    "        targets = tf.reshape(targets, [-1, 4])\n",
    "\n",
    "        # On calcule le masque des atomes définis selon les cibles\n",
    "        defined_atoms_mask = calcul_masque_atomes_definis(targets)\n",
    "        \n",
    "        # On masque les prédictions et les étiquettes selon le masque des atomes définis\n",
    "        targets_masked = tf.boolean_mask(targets, defined_atoms_mask)\n",
    "        predictions_masked = tf.boolean_mask(predictions, defined_atoms_mask)   \n",
    "\n",
    "        return tf.sqrt(tf.reduce_mean(tf.squared_difference(predictions_masked, targets_masked)), name=\"rmse\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pénalisation des modèles effectuant des prédictions avec un écart type faible\n",
    "\n",
    "Afin d'éviter que le modèle prédise systématiquement les mêmes valeurs, nous allons pénaliser les modèles prédisant des valeurs trop proches les unes des autres comme c'est le cas actuellement. La fonction de coût que nous définissons ici est définie de la façon suivante : coût(pred) = RMSE(pred)\\*(1+(1/σ)), σ étant l'écart type de la prédiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_rmse_std_dev_penalty(predictions, targets):\n",
    "    \n",
    "    # On calcule le masque des atomes définis selon les cibles\n",
    "    defined_atoms_mask = calcul_masque_atomes_definis(targets)\n",
    "    predictions_masked = tf.boolean_mask(predictions, defined_atoms_mask)   \n",
    "    \n",
    "    # On calcule la moyenne des prédictions\n",
    "    mean = tf.reduce_mean(predictions_masked)\n",
    "    \n",
    "    # On calcule l'écart type\n",
    "    stddev = tf.sqrt(tf.reduce_mean(tf.squared_difference(predictions_masked, mean)))\n",
    "    \n",
    "    tf.al\n",
    "    \n",
    "    stddev = tf.Print(stddev, [stddev])\n",
    "    \n",
    "    # On calcule le coefficient que l'on ajoute au RMSE selon notre formule\n",
    "    coef = tf.ones_like(stddev) + tf.divide(tf.ones_like(stddev), stddev)\n",
    "    \n",
    "    coef = tf.Print(coef, [coef])\n",
    "    \n",
    "    # On calcul le coût selon notre formule\n",
    "    return tf.multiply(partial_rmse(predictions, targets, None), coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation des données\n",
    "\n",
    "\n",
    "#### Fonction renvoyant deux sous-ensembles du jeu d'entrainement : un ensemble d'exemples et les cibles correspondantes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold(train_set, targets, reduce_train_fold_size):\n",
    "    \"\"\" Permet d'obtenir un sous-ensemble du jeu d'entraînement afin de ne pas travailler sur le jeu\n",
    "    d'entraînement total pour la recherche par quadrillage et donc de gagner du temps d'exécution. L'idée\n",
    "    et que si un ensemble d'hyper-paramètres produit des meilleurs résultats que les autres ensembles\n",
    "    d'hyper-paramètres sur l'ensemble du jeu d'entraînement, alors on suppose que ce sera également \n",
    "    le cas sur une partie des données. \"\"\"\n",
    "\n",
    "    return (train_set[\"inputs\"][:reduce_train_fold_size], targets[\"targets\"][:reduce_train_fold_size])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement des modèles\n",
    "\n",
    "On va ici entraîner des modèles utilisant les fonctions de coût définies pour évaluer leurs performances, toujours en prenant le RMSE partiel utilisé dans les modèles précédents comme référence\n",
    "\n",
    "### Création du RN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.optimizers import Adam\n",
    "from tflearn.data_preprocessing import DataPreprocessing\n",
    "import math\n",
    "\n",
    "\n",
    "def creer_RN(epsilon=1e-8, learning_rate=0.001, dropout_val=0.99, stddev_init=0.001,\n",
    "             hidden_act='relu', outlayer_act='prelu', weight_decay=0.001, width=360, depth=3,\n",
    "             validation_fun=partial_rmse, cost_fun=partial_rmse_std_dev_penalty):\n",
    "    \"\"\" Fonction créant un réseau de neurones de type fully connected, ayant une couche d'entrée de 360\n",
    "    neurones, quatre couches cachées de 360 neurones et une sortie de 240 neurones\n",
    "    Inputs : hyperparamètres\n",
    "    \"\"\"\n",
    "\n",
    "    # On créé l'initialisateur de tenseur avec une loi normale tronquée. sigma = stddev_init, et les \n",
    "    # valeurs à plus de 2sigma sont re-tirées\n",
    "    winit = tfl.initializations.truncated_normal(stddev=stddev_init, dtype=tf.float32, seed=None)\n",
    "    \n",
    "    # On créé l'input du RN\n",
    "    network = input_data(shape=[None, 360], name='input')\n",
    "    \n",
    "    # On créé les couches cachées\n",
    "    for i in range(depth):\n",
    "        network = fully_connected(network, width, activation=hidden_act, name='fc'+str(i), weights_init=winit,\n",
    "                                  weight_decay=weight_decay)\n",
    "        # On détruit des neurones aléatoirement avec une la probabilité donnée en entrée\n",
    "        network = dropout(network, dropout_val)\n",
    "    \n",
    "    # On ajoute la couche de sortie du réseau\n",
    "    # Fonction d'activation prelu\n",
    "    # Initilisée avec la loi normale tronquée\n",
    "    network = fully_connected(network, 240, activation=outlayer_act, name='outlayer', weights_init=winit)\n",
    "    \n",
    "    adam = Adam(learning_rate=learning_rate, epsilon=epsilon)\n",
    "    \n",
    "    # Couche d'évaluation du modèle. Utilisation d'une descente stochastique Adam\n",
    "    # Learning rate = 0.05\n",
    "    # Loss = fonction définie rmse\n",
    "    network = regression(network, optimizer=adam,\n",
    "    loss=cost_fun, metric=validation_fun, name='target')\n",
    "            \n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Définition d'une fonction d'entraînement d'un modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import tflearn as tfl\n",
    "import time\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def train_model(input_X, labels_y, model_name, model_path, logs_path, samples_per_batch=1000, epochs=5,\n",
    "                learning_rate=0.001, epsilon=1e-8, dropout=0.99, stddev_init=0.001, hidden_act='relu',\n",
    "                outlayer_act='prelu', cost_fun=partial_rmse_std_dev_penalty, validation_fun=partial_rmse):\n",
    "    \n",
    "    total_start_time = time.time()\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # On créé le réseau \n",
    "    network = creer_RN(learning_rate=learning_rate, epsilon=epsilon, dropout_val=dropout,\n",
    "                       stddev_init=stddev_init, hidden_act=hidden_act, outlayer_act=outlayer_act, width=1000,\n",
    "                       validation_fun=validation_fun, cost_fun=cost_fun)\n",
    "\n",
    "    # On créé le modèle\n",
    "    model = tfl.DNN(network, tensorboard_verbose=3, tensorboard_dir=logs_path)\n",
    "\n",
    "    # Entraînement\n",
    "    model.fit(X_inputs=input_X,Y_targets=labels_y, batch_size=samples_per_batch,\n",
    "              shuffle = True, snapshot_step=100, validation_set=0.1,\n",
    "              show_metric=True, run_id=model_name, n_epoch=epochs)\n",
    "\n",
    "    # Sauvegarde du modèle\n",
    "    #model.save(model_path + model_name + \".tflearn\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entraînement du modèle pénalisant les prédictions avec un faible écart-type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/etudiant/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Run id: DELTA_DIST+H_05_cost_stddev_penalty\n",
      "Log directory: ../models/DELTA_DIST+H_05/8.5/\n",
      "INFO:tensorflow:Summary name partial_rmse/ (raw) is illegal; using partial_rmse/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 1\n",
      "Validation samples: 1\n",
      "--\n",
      "Training Step: 1  | time: 1.564s\n",
      "| Adam | epoch: 001 | loss: 0.00000 - partial_rmse/rmse: 0.0000 | val_loss: 227.81790 - val_acc: 196.7698 -- iter: 1/1\n",
      "--\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m438156.31250\u001b[0m\u001b[0m | time: 1.119s\n",
      "| Adam | epoch: 002 | loss: 438156.31250 - partial_rmse/rmse: 141.9335 | val_loss: 714.72870 - val_acc: 713.7233 -- iter: 1/1\n",
      "--\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m79833.18750\u001b[0m\u001b[0m | time: 1.104s\n",
      "| Adam | epoch: 003 | loss: 79833.18750 - partial_rmse/rmse: 155.6030 | val_loss: 1558.45740 - val_acc: 1557.6603 -- iter: 1/1\n",
      "--\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m20225.46875\u001b[0m\u001b[0m | time: 1.102s\n",
      "| Adam | epoch: 004 | loss: 20225.46875 - partial_rmse/rmse: 305.3456 | val_loss: 2126.01099 - val_acc: 2125.2114 -- iter: 1/1\n",
      "--\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m6475.67676\u001b[0m\u001b[0m | time: 1.092s\n",
      "| Adam | epoch: 005 | loss: 6475.67676 - partial_rmse/rmse: 346.1632 | val_loss: 1804.19128 - val_acc: 1803.4047 -- iter: 1/1\n",
      "--\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m2655.45020\u001b[0m\u001b[0m | time: 1.098s\n",
      "| Adam | epoch: 006 | loss: 2655.45020 - partial_rmse/rmse: 466.1019 | val_loss: 1151.91711 - val_acc: 1151.1038 -- iter: 1/1\n",
      "--\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m1263.15747\u001b[0m\u001b[0m | time: 1.115s\n",
      "| Adam | epoch: 007 | loss: 1263.15747 - partial_rmse/rmse: 387.2475 | val_loss: 463.37677 - val_acc: 462.4876 -- iter: 1/1\n",
      "--\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m659.22314\u001b[0m\u001b[0m | time: 1.103s\n",
      "| Adam | epoch: 008 | loss: 659.22314 - partial_rmse/rmse: 275.8747 | val_loss: 581.19183 - val_acc: 580.3413 -- iter: 1/1\n",
      "--\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m356.81177\u001b[0m\u001b[0m | time: 1.157s\n",
      "| Adam | epoch: 009 | loss: 356.81177 - partial_rmse/rmse: 176.2421 | val_loss: 221.71782 - val_acc: 220.1481 -- iter: 1/1\n",
      "--\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m221.57254\u001b[0m\u001b[0m | time: 1.101s\n",
      "| Adam | epoch: 010 | loss: 221.57254 - partial_rmse/rmse: 131.1692 | val_loss: 370.42892 - val_acc: 369.5048 -- iter: 1/1\n",
      "--\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m166.09700\u001b[0m\u001b[0m | time: 1.089s\n",
      "| Adam | epoch: 011 | loss: 166.09700 - partial_rmse/rmse: 117.8342 | val_loss: 246.23653 - val_acc: 244.9671 -- iter: 1/1\n",
      "--\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m114.16582\u001b[0m\u001b[0m | time: 1.094s\n",
      "| Adam | epoch: 012 | loss: 114.16582 - partial_rmse/rmse: 87.5227 | val_loss: 378.73080 - val_acc: 377.7898 -- iter: 1/1\n",
      "--\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m96.95937\u001b[0m\u001b[0m | time: 1.089s\n",
      "| Adam | epoch: 013 | loss: 96.95937 - partial_rmse/rmse: 81.4032 | val_loss: 272.82440 - val_acc: 271.6623 -- iter: 1/1\n",
      "--\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m75.48277\u001b[0m\u001b[0m | time: 1.093s\n",
      "| Adam | epoch: 014 | loss: 75.48277 - partial_rmse/rmse: 66.2027 | val_loss: 400.56830 - val_acc: 399.6095 -- iter: 1/1\n",
      "--\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m60.06725\u001b[0m\u001b[0m | time: 1.095s\n",
      "| Adam | epoch: 015 | loss: 60.06725 - partial_rmse/rmse: 54.3057 | val_loss: 304.91632 - val_acc: 303.8356 -- iter: 1/1\n",
      "--\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m64.72260\u001b[0m\u001b[0m | time: 1.082s\n",
      "| Adam | epoch: 016 | loss: 64.72260 - partial_rmse/rmse: 60.9971 | val_loss: 210.14601 - val_acc: 208.1442 -- iter: 1/1\n",
      "--\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m47.58165\u001b[0m\u001b[0m | time: 1.099s\n",
      "| Adam | epoch: 017 | loss: 47.58165 - partial_rmse/rmse: 45.1562 | val_loss: 223.09683 - val_acc: 221.5785 -- iter: 1/1\n",
      "--\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m65.38480\u001b[0m\u001b[0m | time: 1.110s\n",
      "| Adam | epoch: 018 | loss: 65.38480 - partial_rmse/rmse: 63.1947 | val_loss: 360.55194 - val_acc: 359.5976 -- iter: 1/1\n",
      "--\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m70.50723\u001b[0m\u001b[0m | time: 1.092s\n",
      "| Adam | epoch: 019 | loss: 70.50723 - partial_rmse/rmse: 68.6858 | val_loss: 385.56888 - val_acc: 384.6310 -- iter: 1/1\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m75.43075\u001b[0m\u001b[0m | time: 1.100s\n",
      "| Adam | epoch: 020 | loss: 75.43075 - partial_rmse/rmse: 74.0544 | val_loss: 279.04395 - val_acc: 277.9545 -- iter: 1/1\n",
      "--\n",
      "Training Step: 21  | total loss: \u001b[1m\u001b[32m82.06020\u001b[0m\u001b[0m | time: 1.084s\n",
      "| Adam | epoch: 021 | loss: 82.06020 - partial_rmse/rmse: 80.9598 | val_loss: 247.35699 - val_acc: 246.1153 -- iter: 1/1\n",
      "--\n",
      "Training Step: 22  | total loss: \u001b[1m\u001b[32m72.09605\u001b[0m\u001b[0m | time: 1.089s\n",
      "| Adam | epoch: 022 | loss: 72.09605 - partial_rmse/rmse: 71.2128 | val_loss: 305.55771 - val_acc: 304.5060 -- iter: 1/1\n",
      "--\n",
      "Training Step: 23  | total loss: \u001b[1m\u001b[32m64.20390\u001b[0m\u001b[0m | time: 1.084s\n",
      "| Adam | epoch: 023 | loss: 64.20390 - partial_rmse/rmse: 63.4497 | val_loss: 281.34064 - val_acc: 280.1885 -- iter: 1/1\n",
      "--\n",
      "Training Step: 24  | total loss: \u001b[1m\u001b[32m49.38910\u001b[0m\u001b[0m | time: 1.097s\n",
      "| Adam | epoch: 024 | loss: 49.38910 - partial_rmse/rmse: 48.8241 | val_loss: 340.60950 - val_acc: 339.5611 -- iter: 1/1\n",
      "--\n",
      "Training Step: 25  | total loss: \u001b[1m\u001b[32m40.67725\u001b[0m\u001b[0m | time: 1.096s\n",
      "| Adam | epoch: 025 | loss: 40.67725 - partial_rmse/rmse: 40.2280 | val_loss: 300.94241 - val_acc: 299.8146 -- iter: 1/1\n",
      "--\n",
      "Training Step: 26  | total loss: \u001b[1m\u001b[32m43.51933\u001b[0m\u001b[0m | time: 1.093s\n",
      "| Adam | epoch: 026 | loss: 43.51933 - partial_rmse/rmse: 43.1090 | val_loss: 226.00143 - val_acc: 224.3707 -- iter: 1/1\n",
      "--\n",
      "Training Step: 27  | total loss: \u001b[1m\u001b[32m40.24704\u001b[0m\u001b[0m | time: 1.113s\n",
      "| Adam | epoch: 027 | loss: 40.24704 - partial_rmse/rmse: 39.8874 | val_loss: 220.20052 - val_acc: 218.4436 -- iter: 1/1\n",
      "--\n",
      "Training Step: 28  | total loss: \u001b[1m\u001b[32m49.53397\u001b[0m\u001b[0m | time: 1.090s\n",
      "| Adam | epoch: 028 | loss: 49.53397 - partial_rmse/rmse: 48.9753 | val_loss: 248.73775 - val_acc: 247.4041 -- iter: 1/1\n",
      "--\n",
      "Training Step: 29  | total loss: \u001b[1m\u001b[32m55.91992\u001b[0m\u001b[0m | time: 1.088s\n",
      "| Adam | epoch: 029 | loss: 55.91992 - partial_rmse/rmse: 55.2297 | val_loss: 359.36807 - val_acc: 358.3541 -- iter: 1/1\n",
      "--\n",
      "Training Step: 30  | total loss: \u001b[1m\u001b[32m52.96103\u001b[0m\u001b[0m | time: 1.105s\n",
      "| Adam | epoch: 030 | loss: 52.96103 - partial_rmse/rmse: 52.3258 | val_loss: 409.50351 - val_acc: 408.5339 -- iter: 1/1\n",
      "--\n",
      "Training Step: 31  | total loss: \u001b[1m\u001b[32m58.00801\u001b[0m\u001b[0m | time: 1.130s\n",
      "| Adam | epoch: 031 | loss: 58.00801 - partial_rmse/rmse: 57.4246 | val_loss: 363.32333 - val_acc: 362.3271 -- iter: 1/1\n",
      "--\n",
      "Training Step: 32  | total loss: \u001b[1m\u001b[32m71.93444\u001b[0m\u001b[0m | time: 1.133s\n",
      "| Adam | epoch: 032 | loss: 71.93444 - partial_rmse/rmse: 71.3587 | val_loss: 272.19586 - val_acc: 271.0313 -- iter: 1/1\n",
      "--\n",
      "Training Step: 33  | total loss: \u001b[1m\u001b[32m68.78487\u001b[0m\u001b[0m | time: 1.091s\n",
      "| Adam | epoch: 033 | loss: 68.78487 - partial_rmse/rmse: 68.2613 | val_loss: 259.54819 - val_acc: 258.3376 -- iter: 1/1\n",
      "--\n",
      "Training Step: 34  | total loss: \u001b[1m\u001b[32m56.63610\u001b[0m\u001b[0m | time: 1.087s\n",
      "| Adam | epoch: 034 | loss: 56.63610 - partial_rmse/rmse: 56.2032 | val_loss: 297.73407 - val_acc: 296.6592 -- iter: 1/1\n",
      "--\n",
      "Training Step: 35  | total loss: \u001b[1m\u001b[32m47.45189\u001b[0m\u001b[0m | time: 1.093s\n",
      "| Adam | epoch: 035 | loss: 47.45189 - partial_rmse/rmse: 47.0868 | val_loss: 284.52646 - val_acc: 283.4199 -- iter: 1/1\n",
      "--\n",
      "Training Step: 36  | total loss: \u001b[1m\u001b[32m42.93897\u001b[0m\u001b[0m | time: 1.092s\n",
      "| Adam | epoch: 036 | loss: 42.93897 - partial_rmse/rmse: 42.6122 | val_loss: 238.66530 - val_acc: 237.3234 -- iter: 1/1\n",
      "--\n",
      "Training Step: 37  | total loss: \u001b[1m\u001b[32m38.07854\u001b[0m\u001b[0m | time: 1.119s\n",
      "| Adam | epoch: 037 | loss: 38.07854 - partial_rmse/rmse: 37.7890 | val_loss: 238.10443 - val_acc: 236.7540 -- iter: 1/1\n",
      "--\n",
      "Training Step: 38  | total loss: \u001b[1m\u001b[32m39.32486\u001b[0m\u001b[0m | time: 1.126s\n",
      "| Adam | epoch: 038 | loss: 39.32486 - partial_rmse/rmse: 38.9986 | val_loss: 269.36255 - val_acc: 268.1997 -- iter: 1/1\n",
      "--\n",
      "Training Step: 39  | total loss: \u001b[1m\u001b[32m50.09295\u001b[0m\u001b[0m | time: 1.084s\n",
      "| Adam | epoch: 039 | loss: 50.09295 - partial_rmse/rmse: 49.4801 | val_loss: 369.09491 - val_acc: 368.1084 -- iter: 1/1\n",
      "--\n",
      "Training Step: 40  | total loss: \u001b[1m\u001b[32m43.83654\u001b[0m\u001b[0m | time: 1.082s\n",
      "| Adam | epoch: 040 | loss: 43.83654 - partial_rmse/rmse: 43.3126 | val_loss: 416.45844 - val_acc: 415.5008 -- iter: 1/1\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 41  | total loss: \u001b[1m\u001b[32m54.71764\u001b[0m\u001b[0m | time: 1.082s\n",
      "| Adam | epoch: 041 | loss: 54.71764 - partial_rmse/rmse: 54.1972 | val_loss: 386.62140 - val_acc: 385.6464 -- iter: 1/1\n",
      "--\n",
      "Training Step: 42  | total loss: \u001b[1m\u001b[32m64.41997\u001b[0m\u001b[0m | time: 1.127s\n",
      "| Adam | epoch: 042 | loss: 64.41997 - partial_rmse/rmse: 63.9029 | val_loss: 306.71939 - val_acc: 305.6500 -- iter: 1/1\n",
      "--\n",
      "Training Step: 43  | total loss: \u001b[1m\u001b[32m69.29197\u001b[0m\u001b[0m | time: 1.102s\n",
      "| Adam | epoch: 043 | loss: 69.29197 - partial_rmse/rmse: 68.7869 | val_loss: 232.82098 - val_acc: 231.3691 -- iter: 1/1\n",
      "--\n",
      "Training Step: 44  | total loss: \u001b[1m\u001b[32m62.49785\u001b[0m\u001b[0m | time: 1.082s\n",
      "| Adam | epoch: 044 | loss: 62.49785 - partial_rmse/rmse: 62.0460 | val_loss: 216.76106 - val_acc: 214.9614 -- iter: 1/1\n",
      "--\n",
      "Training Step: 45  | total loss: \u001b[1m\u001b[32m61.21859\u001b[0m\u001b[0m | time: 1.116s\n",
      "| Adam | epoch: 045 | loss: 61.21859 - partial_rmse/rmse: 60.7358 | val_loss: 218.88394 - val_acc: 217.1404 -- iter: 1/1\n",
      "--\n",
      "Training Step: 46  | total loss: \u001b[1m\u001b[32m64.85848\u001b[0m\u001b[0m | time: 1.085s\n",
      "| Adam | epoch: 046 | loss: 64.85848 - partial_rmse/rmse: 64.2409 | val_loss: 239.07307 - val_acc: 237.6810 -- iter: 1/1\n",
      "--\n",
      "Training Step: 47  | total loss: \u001b[1m\u001b[32m67.33034\u001b[0m\u001b[0m | time: 1.094s\n",
      "| Adam | epoch: 047 | loss: 67.33034 - partial_rmse/rmse: 66.6211 | val_loss: 299.70581 - val_acc: 298.6191 -- iter: 1/1\n",
      "--\n",
      "Training Step: 48  | total loss: \u001b[1m\u001b[32m69.57497\u001b[0m\u001b[0m | time: 1.082s\n",
      "| Adam | epoch: 048 | loss: 69.57497 - partial_rmse/rmse: 68.7852 | val_loss: 333.69098 - val_acc: 332.6701 -- iter: 1/1\n",
      "--\n",
      "Training Step: 49  | total loss: \u001b[1m\u001b[32m61.07202\u001b[0m\u001b[0m | time: 1.105s\n",
      "| Adam | epoch: 049 | loss: 61.07202 - partial_rmse/rmse: 60.3894 | val_loss: 320.07309 - val_acc: 319.0365 -- iter: 1/1\n",
      "--\n",
      "Training Step: 50  | total loss: \u001b[1m\u001b[32m59.71919\u001b[0m\u001b[0m | time: 1.084s\n",
      "| Adam | epoch: 050 | loss: 59.71919 - partial_rmse/rmse: 59.0966 | val_loss: 275.62817 - val_acc: 274.4958 -- iter: 1/1\n",
      "--\n",
      "Training Step: 51  | total loss: \u001b[1m\u001b[32m51.50951\u001b[0m\u001b[0m | time: 1.089s\n",
      "| Adam | epoch: 051 | loss: 51.50951 - partial_rmse/rmse: 50.9753 | val_loss: 272.60495 - val_acc: 271.4736 -- iter: 1/1\n",
      "--\n",
      "Training Step: 52  | total loss: \u001b[1m\u001b[32m47.63356\u001b[0m\u001b[0m | time: 1.083s\n",
      "| Adam | epoch: 052 | loss: 47.63356 - partial_rmse/rmse: 47.1461 | val_loss: 302.40579 - val_acc: 301.3622 -- iter: 1/1\n",
      "--\n",
      "Training Step: 53  | total loss: \u001b[1m\u001b[32m44.49355\u001b[0m\u001b[0m | time: 1.081s\n",
      "| Adam | epoch: 053 | loss: 44.49355 - partial_rmse/rmse: 44.0452 | val_loss: 381.50238 - val_acc: 380.5492 -- iter: 1/1\n",
      "--\n",
      "Training Step: 54  | total loss: \u001b[1m\u001b[32m41.04380\u001b[0m\u001b[0m | time: 1.096s\n",
      "| Adam | epoch: 054 | loss: 41.04380 - partial_rmse/rmse: 40.6362 | val_loss: 414.00336 - val_acc: 413.0660 -- iter: 1/1\n",
      "--\n",
      "Training Step: 55  | total loss: \u001b[1m\u001b[32m48.95740\u001b[0m\u001b[0m | time: 1.075s\n",
      "| Adam | epoch: 055 | loss: 48.95740 - partial_rmse/rmse: 48.5443 | val_loss: 391.93771 - val_acc: 390.9863 -- iter: 1/1\n",
      "--\n",
      "Training Step: 56  | total loss: \u001b[1m\u001b[32m48.20970\u001b[0m\u001b[0m | time: 1.089s\n",
      "| Adam | epoch: 056 | loss: 48.20970 - partial_rmse/rmse: 47.8176 | val_loss: 334.99905 - val_acc: 333.9945 -- iter: 1/1\n",
      "--\n",
      "Training Step: 57  | total loss: \u001b[1m\u001b[32m47.69834\u001b[0m\u001b[0m | time: 1.129s\n",
      "| Adam | epoch: 057 | loss: 47.69834 - partial_rmse/rmse: 47.3243 | val_loss: 270.70709 - val_acc: 269.5408 -- iter: 1/1\n",
      "--\n",
      "Training Step: 58  | total loss: \u001b[1m\u001b[32m42.46617\u001b[0m\u001b[0m | time: 1.098s\n",
      "| Adam | epoch: 058 | loss: 42.46617 - partial_rmse/rmse: 42.1347 | val_loss: 253.59535 - val_acc: 252.3208 -- iter: 1/1\n",
      "--\n",
      "Training Step: 59  | total loss: \u001b[1m\u001b[32m43.32379\u001b[0m\u001b[0m | time: 1.098s\n",
      "| Adam | epoch: 059 | loss: 43.32379 - partial_rmse/rmse: 42.9748 | val_loss: 260.39108 - val_acc: 259.1495 -- iter: 1/1\n",
      "--\n",
      "Training Step: 60  | total loss: \u001b[1m\u001b[32m43.68060\u001b[0m\u001b[0m | time: 1.082s\n",
      "| Adam | epoch: 060 | loss: 43.68060 - partial_rmse/rmse: 43.3214 | val_loss: 290.86218 - val_acc: 289.7450 -- iter: 1/1\n",
      "--\n",
      "Training Step: 61  | total loss: \u001b[1m\u001b[32m44.19301\u001b[0m\u001b[0m | time: 1.111s\n",
      "| Adam | epoch: 061 | loss: 44.19301 - partial_rmse/rmse: 43.8238 | val_loss: 358.52423 - val_acc: 357.5328 -- iter: 1/1\n",
      "--\n",
      "Training Step: 62  | total loss: \u001b[1m\u001b[32m40.80986\u001b[0m\u001b[0m | time: 1.098s\n",
      "| Adam | epoch: 062 | loss: 40.80986 - partial_rmse/rmse: 40.4712 | val_loss: 479.19998 - val_acc: 478.2772 -- iter: 1/1\n",
      "--\n",
      "Training Step: 63  | total loss: \u001b[1m\u001b[32m46.35582\u001b[0m\u001b[0m | time: 1.084s\n",
      "| Adam | epoch: 063 | loss: 46.35582 - partial_rmse/rmse: 45.9103 | val_loss: 554.42249 - val_acc: 553.5165 -- iter: 1/1\n",
      "--\n",
      "Training Step: 64  | total loss: \u001b[1m\u001b[32m60.38287\u001b[0m\u001b[0m | time: 1.168s\n",
      "| Adam | epoch: 064 | loss: 60.38287 - partial_rmse/rmse: 59.9222 | val_loss: 561.82471 - val_acc: 560.9227 -- iter: 1/1\n",
      "--\n",
      "Training Step: 65  | total loss: \u001b[1m\u001b[32m78.31206\u001b[0m\u001b[0m | time: 1.080s\n",
      "| Adam | epoch: 065 | loss: 78.31206 - partial_rmse/rmse: 77.8262 | val_loss: 507.06903 - val_acc: 506.1626 -- iter: 1/1\n",
      "--\n",
      "Training Step: 66  | total loss: \u001b[1m\u001b[32m94.83205\u001b[0m\u001b[0m | time: 1.077s\n",
      "| Adam | epoch: 066 | loss: 94.83205 - partial_rmse/rmse: 94.3263 | val_loss: 415.04431 - val_acc: 414.1148 -- iter: 1/1\n",
      "--\n",
      "Training Step: 67  | total loss: \u001b[1m\u001b[32m104.82819\u001b[0m\u001b[0m | time: 1.084s\n",
      "| Adam | epoch: 067 | loss: 104.82819 - partial_rmse/rmse: 104.3106 | val_loss: 318.15723 - val_acc: 317.1476 -- iter: 1/1\n",
      "--\n",
      "Training Step: 68  | total loss: \u001b[1m\u001b[32m102.39127\u001b[0m\u001b[0m | time: 1.092s\n",
      "| Adam | epoch: 068 | loss: 102.39127 - partial_rmse/rmse: 101.8914 | val_loss: 246.83012 - val_acc: 245.5555 -- iter: 1/1\n",
      "--\n",
      "Training Step: 69  | total loss: \u001b[1m\u001b[32m92.91249\u001b[0m\u001b[0m | time: 1.086s\n",
      "| Adam | epoch: 069 | loss: 92.91249 - partial_rmse/rmse: 92.4566 | val_loss: 224.11237 - val_acc: 222.5106 -- iter: 1/1\n",
      "--\n",
      "Training Step: 70  | total loss: \u001b[1m\u001b[32m89.13937\u001b[0m\u001b[0m | time: 1.089s\n",
      "| Adam | epoch: 070 | loss: 89.13937 - partial_rmse/rmse: 88.6694 | val_loss: 218.62045 - val_acc: 216.8378 -- iter: 1/1\n",
      "--\n",
      "Training Step: 71  | total loss: \u001b[1m\u001b[32m89.18252\u001b[0m\u001b[0m | time: 1.101s\n",
      "| Adam | epoch: 071 | loss: 89.18252 - partial_rmse/rmse: 88.6339 | val_loss: 221.34795 - val_acc: 219.6320 -- iter: 1/1\n",
      "--\n",
      "Training Step: 72  | total loss: \u001b[1m\u001b[32m90.84287\u001b[0m\u001b[0m | time: 1.088s\n",
      "| Adam | epoch: 072 | loss: 90.84287 - partial_rmse/rmse: 90.1683 | val_loss: 233.20961 - val_acc: 231.7306 -- iter: 1/1\n",
      "--\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-f404726cc486>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m train_model(input_X, labels_y, model_name, model_path, logs_path, samples_per_batch=1000, \n\u001b[1;32m     18\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.97\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_act\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"elu\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             outlayer_act=\"linear\", validation_fun=partial_rmse, cost_fun=partial_rmse_std_dev_penalty)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-58-76cc5981dcf7>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(input_X, labels_y, model_name, model_path, logs_path, samples_per_batch, epochs, learning_rate, epsilon, dropout, stddev_init, hidden_act, outlayer_act, cost_fun, validation_fun)\u001b[0m\n\u001b[1;32m     27\u001b[0m     model.fit(X_inputs=input_X,Y_targets=labels_y, batch_size=samples_per_batch,\n\u001b[1;32m     28\u001b[0m               \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnapshot_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m               show_metric=True, run_id=model_name, n_epoch=epochs)\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Sauvegarde du modèle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tflearn/models/dnn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_inputs, Y_targets, n_epoch, validation_set, show_metric, batch_size, shuffle, snapshot_epoch, snapshot_step, excl_trainops, validation_batch_size, run_id, callbacks)\u001b[0m\n\u001b[1;32m    204\u001b[0m                          \u001b[0mexcl_trainops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexcl_trainops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                          \u001b[0mrun_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                          callbacks=callbacks)\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mretrieve_data_preprocessing_and_augmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tflearn/helpers/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, feed_dicts, n_epoch, val_feed_dicts, show_metric, snapshot_step, snapshot_epoch, shuffle_all, dprep_dict, daug_dict, excl_trainops, run_id, callbacks)\u001b[0m\n\u001b[1;32m    336\u001b[0m                                                        \u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0msnapshot_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m                                                        \u001b[0msnapshot_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                                                        show_metric)\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m                             \u001b[0;31m# Update training state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tflearn/helpers/trainer.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, training_step, snapshot_epoch, snapshot_step, show_metric)\u001b[0m\n\u001b[1;32m    844\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshow_metric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0meval_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m             \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_flow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dflow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshow_metric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tflearn/helpers/trainer.py\u001b[0m in \u001b[0;36mevaluate_flow\u001b[0;34m(session, ops_to_evaluate, dataflow)\u001b[0m\n\u001b[1;32m    999\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m                 \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcurrent_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1001\u001b[0;31m             \u001b[0mfeed_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdataflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_samples\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tflearn/data_flow.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \"\"\"\n\u001b[1;32m    128\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_status\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_dict_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "\n",
    "input_X_h5 = h5py.File(train_prepared_input_loc, 'r')\n",
    "labels_y_h5 = h5py.File(train_labels_loc, 'r')\n",
    "\n",
    "\"\"\"\n",
    "input_X_h5 = h5py.File(mini_prepared_input_loc, 'r')\n",
    "labels_y_h5 = h5py.File(mini_labels_loc, 'r')\n",
    "\"\"\"\n",
    "input_X, labels_y = get_fold(input_X_h5, labels_y_h5, 5000000)\n",
    "\n",
    "model_name = \"DELTA_DIST+H_05_cost_stddev_penalty\"\n",
    "model_path = models_loc\n",
    "logs_path = logs_loc\n",
    "\n",
    "train_model(input_X, labels_y, model_name, model_path, logs_path, samples_per_batch=1000, \n",
    "            epochs=100, learning_rate=0.01, dropout=0.97, epsilon=0.001, hidden_act=\"elu\",\n",
    "            outlayer_act=\"linear\", validation_fun=partial_rmse, cost_fun=partial_rmse_std_dev_penalty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
