{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Séparation du jeu\n",
    "\n",
    "L'objectif de ce notebook est de séparer le jeu de données en un jeu d'entraînement (90% des données) et un jeu de validation (10% des données).\n",
    "Le code de ce notebook sera extrait et exécuté sur la machine Tesla afin de profiter du CPU et de la mémoire disponible pour créer deux nouveaux fichiers h5 tout en gardant le dataset original en mémoire et intact.\n",
    "La séparation du jeu en fichiers distincts permettra de s'assurer facilement par la suite que l'on s'entraîne uniquement sur le jeu d'entraînement et qu'on ne voit jamais les données de validation avant la phase de test\n",
    "\n",
    "Une fois le jeu séparé en un jeu d'entraînement et un jeu de validation, on créé un sous ensemble du jeu d'entraînement sur lequel on va pouvoir par la suite faire les tests d'entraînement de modèles avant de lancer l'entraînement des modèles sur l'ensemble d'un jeu d'entraînement. On note que de cette façon on n'aura jamais vu les données du jeu de test.\n",
    "\n",
    "### Définition des constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset_filename = \"../data/riken_v2.h5\"\n",
    "train_dataset_filename = \"../data/train_set_riken_v2.h5\"\n",
    "test_dataset_filename = \"../data/test_set_riken_v2.h5\"\n",
    "minimal_dataset_filename = \"../data/minimal_set_riken_v2.h5\"\n",
    "test_size = 0.1\n",
    "minimal_dataset_size = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Séparation jeu d'entraînement/jeu de test\n",
    "\n",
    "#### Définition de la fonction de séparation du jeu original en un jeu d'entraînement et un jeu de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def split_jeux(original_dataset_filename, train_dataset_filename, test_dataset_filename, test_size=0.1):\n",
    "    \"\"\" Fonction prenant un dataset h5 en entrée et le séparant en un jeu d'entraînement et un jeu de test,\n",
    "    qu'elle enregistre\"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # On charge le jeu de données original (en lecture seule)\n",
    "    original_dataset = h5py.File(original_dataset_filename, 'r')  \n",
    "\n",
    "    # On créé un tableau numpy contenant toutes les données\n",
    "    print(\"Loading the data...\")\n",
    "    data = np.array([np.int32(original_dataset[\"pubchem_id\"]), \n",
    "              np.asarray(original_dataset[\"anums\"]),\n",
    "              np.asarray(original_dataset[\"amasses\"]),\n",
    "              np.asarray(original_dataset[\"riken_coords\"])]).T\n",
    "    \n",
    "    # On fait appel à scikit learn pour séparer les données de façon aléatoire\n",
    "    print(\"Separation of the data...\")\n",
    "    train_set, test_set = train_test_split(data, test_size=test_size, random_state = 12)\n",
    "    \n",
    "    \n",
    "    # On créé deux fichiers h5\n",
    "    print(\"Creating h5 files...\")\n",
    "    train_set_h5 = h5py.File(train_dataset_filename, 'w')\n",
    "    test_set_h5 = h5py.File(test_dataset_filename, 'w')\n",
    "\n",
    "    try:\n",
    "        # Définition du type pour les tableaux de floats\n",
    "        varlen_floatarray = h5py.special_dtype(vlen=np.dtype(\"float32\"))\n",
    "\n",
    "        # Calcul de la taille des deux datasets de sortie\n",
    "        train_size = len(train_set)\n",
    "        test_size = len(test_set)\n",
    "        \n",
    "        col_names = [\"pubchem_id\", \"anums\", \"amasses\", \"riken_coords\"]\n",
    "        datatypes = [np.int32, varlen_floatarray, varlen_floatarray, varlen_floatarray]\n",
    "\n",
    "        train_datasets = []\n",
    "        test_datasets = []\n",
    "        \n",
    "        # On créé les quatre datasets (un par colonne) par fichier de sortie\n",
    "        print(\"Creating datasets...\")\n",
    "        for i in range(4):\n",
    "            \n",
    "            train_datasets.append(train_set_h5.create_dataset(col_names[i], shape=(train_size,),\n",
    "                                       dtype=datatypes[i], compression=\"gzip\", \n",
    "                                       chunks=True, maxshape=(None,)))\n",
    "\n",
    "            test_datasets.append(test_set_h5.create_dataset(col_names[i], shape=(test_size,),\n",
    "                                       dtype=datatypes[i], compression=\"gzip\", \n",
    "                                       chunks=True, maxshape=(None,)))\n",
    "\n",
    "            \n",
    "            \n",
    "        # On écrit le dataset d'entraînement en mémoire\n",
    "        print(\"Writing train set in memory\")\n",
    "        for i in range(train_size):\n",
    "            j=0\n",
    "            for col_name in col_names:\n",
    "                train_datasets[j][i] = train_set[i][j]\n",
    "                j += 1\n",
    "        \n",
    "        # On écrit le dataset de test en mémoire\n",
    "        print(\"Writing test set in memory\")\n",
    "        for i in range(test_size):\n",
    "            j=0\n",
    "            for col_name in col_names:\n",
    "                test_datasets[j][i] = test_set[i][j]\n",
    "                j += 1\n",
    "\n",
    "\n",
    "        \n",
    "        # On écrit les datasets sur le disque\n",
    "        print(\"Writing train set to disk...\")\n",
    "        train_set_h5.flush()\n",
    "\n",
    "        print(\"Writing test set to disk...\")\n",
    "        test_set_h5.flush()\n",
    "        \n",
    "        print(\"Succesful creation of a train set and a test set\")\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "        \n",
    "    finally:\n",
    "        # Closing the files\n",
    "        original_dataset.close()\n",
    "        train_set_h5.close()\n",
    "        test_set_h5.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appel à la fonction de séparation du jeu original en un jeu d'entraînement et un jeu de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-300fc495dc68>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-300fc495dc68>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    split_jeux(original_dataset_filename, train_dataset_filename, test_dataset_filename, test_size=test_size):\u001b[0m\n\u001b[0m                                                                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Appel à la fonction de séparation du jeu\n",
    "\n",
    "print(\"SÉPARATION DU JEU ORIGINAL EN UN JEU D'ENTRAÎNEMENT ET UN JEU DE TEST\")\n",
    "split_jeux(original_dataset_filename, train_dataset_filename, test_dataset_filename, test_size=test_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Séparation jeu d'entraînement/jeu d'entraînement minimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Définition de la fonction de séparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def creer_h5_ss_ensemble(original_dataset_filename, dataset_dest_filename, taille):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    # Ouverture du dataset d'entrée en lecture seule\n",
    "    original_dataset =  h5py.File(original_dataset_filename, 'r')\n",
    "    \n",
    "    # Création du dataset minimal de sortie\n",
    "    dataset_reduit_h5 = h5py.File(dataset_dest_filename, 'w')\n",
    "    \n",
    "    try:\n",
    "        # Définition du type spécial\n",
    "        varlen_floatarray = h5py.special_dtype(vlen=np.dtype(\"float32\"))\n",
    "\n",
    "        print(\"Copie des données...\")\n",
    "        \n",
    "        # Copie des premières valeurs de la colonne pubchem_id\n",
    "        dataset_reduit_h5.create_dataset(\"pubchem_id\", shape=(taille,),\n",
    "                                       dtype=np.int32, compression=\"gzip\", \n",
    "                                       chunks=True, maxshape=(None,), \n",
    "                                       data=original_dataset[\"pubchem_id\"][:taille])\n",
    "\n",
    "        # Copie des premières valeurs des autres colonnes\n",
    "        cols = [\"anums\", \"amasses\", \"riken_coords\"]  \n",
    "        for col_name in cols:\n",
    "            dataset_reduit_h5.create_dataset(col_name, shape=(taille,),\n",
    "                                           dtype=varlen_floatarray, compression=\"gzip\", \n",
    "                                           chunks=True, maxshape=(None,), \n",
    "                                           data=original_dataset[col_name][:taille])\n",
    "\n",
    "        dataset_reduit_h5.flush()\n",
    "        print(\"Jeu minimal enregistré avec succès\")\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        \n",
    "    finally:\n",
    "        dataset_reduit_h5.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appel de la fonction de création d'un jeu d'entraînement minimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"CRÉATION JEU D'ENTRAÎNEMENT MINIMAL\")\n",
    "\n",
    "creer_h5_ss_ensemble(train_dataset_filename, minimal_dataset_filename, minimal_dataset_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
