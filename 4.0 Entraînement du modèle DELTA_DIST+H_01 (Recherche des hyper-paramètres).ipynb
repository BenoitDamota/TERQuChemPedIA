{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DELTA_DIST+H - Recherche des hyperparamètres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle DELTA_DIST+H est le premier que l'on va entraîner. L'objectif est d'obtenir un modèle capable d'optimiser des molécules à partir des coordonnées non optimisées des atomes. La méthode d'entraînement est la suivante : on part des coordonnées optimisées des atomes de la molécule, on leur ajoute un bruit gaussien, on calcule les distances des atomes bruités par rapport aux atomes fictifs du repère, puis on tente d'apprendre au modèle à prédire la différence entre les distances des atomes bruités et les distances des atomes optimisés.\n",
    "\n",
    "Dans ce notebook on propose une solution (recherche par quadrillage) pour faire varier les hyperparamètres du modèle et chercher leurs meilleures valeurs pour entraîner ensuite un modèle possédant les hyperparamètres produisant les meilleurs résultats. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chemins des fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimal_prepared_input_loc = \"../data/minimal_set_riken_v2_prepared_input.h5\"\n",
    "minimal_labels_loc = \"../data/minimal_set_riken_v2_labels.h5\"\n",
    "\n",
    "train_prepared_input_loc = \"../data/train_set_riken_v2_prepared_input.h5\"\n",
    "train_labels_loc = \"../data/train_set_riken_v2_labels.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du RN\n",
    "\n",
    "### Fonctions de coût et d'évaluation du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonction renvoyant le masque des atomes à prendre en compte pour les calculs\n",
    "\n",
    "L'entrée et la sortie du RN étant définies par une méthode de padding, seul un certain nombre d'entrées et de sortie est utilisé pour chaque exemple d'entraînement en fonction du nombre d'atomes de la molécule. On définit ici une fonction qui renvoie le masque des différences de distances à prendre en compte sur les données en entrée et les étiquettes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/etudiant/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def calcul_masque_atomes_definis(targets):\n",
    "    \"\"\" On calcule le masque booléen des atomes donnés en entrée du RN en fonction du vecteur targets\"\"\"\n",
    "    \n",
    "    # On cherche à obtenir un masque booléen des atomes définis en entrée. Pour cela, on prend en entrée\n",
    "    # les étiquettes sous la forme d'une matrice (200, 4) dont chaque ligne i est la distance de l'atome i avec\n",
    "    # les atomes fictifs du repère. L'atome est indéfini ssi. la somme de la ligne est nulle. En effet,\n",
    "    # un atome défini ne peut pas avoir une distance nulle avec les quatre atomes fictifs, et on veille\n",
    "    # à ce que le vecteurs targets ne contienne que des valeurs nulles pour les atomes non définis.\n",
    "    # On obtient donc un masque booléen de tous les atomes définis en entrée\n",
    "    \n",
    "    ## On somme les distances de chaque atome ##\n",
    "    targets_dists_sums = tf.reduce_sum(targets, 1)\n",
    "    \n",
    "    ## On créé le masque des sommes différentes de zéro ##\n",
    "    \n",
    "    # Création des matrice de True et de False de la dimension de la matrice des sommes (nécessaires\n",
    "    # pour tf.where)\n",
    "    zeros = tf.cast(tf.zeros_like(targets_dists_sums),dtype=tf.bool)\n",
    "    ones = tf.cast(tf.ones_like(targets_dists_sums),dtype=tf.bool)\n",
    "    \n",
    "    return tf.where(targets_dists_sums>0, ones, zeros)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonction de coût"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_rmse(predictions, targets):\n",
    "    \"\"\" Calcule le RMSE partiel des prédictions par rapport aux valeurs attendues. Le RMSE est partiel car\n",
    "    on ne le calcule que pour les sorties correspondant aux atomes donnés en entrée. En d'autres\n",
    "    termes, on ne pousse pas le modèle à donner des distances nulles pour les atomes indéfinis\n",
    "    en entrée\"\"\"\n",
    "    \n",
    "\n",
    "    # On met les prédictions et les cibles sous la forme d'une matrice (200, 4)\n",
    "    predictions = tf.reshape(predictions, [-1, 4])\n",
    "    targets = tf.reshape(targets, [-1, 4])\n",
    "    \n",
    "    # On calcule le masque des atomes définis selon les cibles\n",
    "    defined_atoms_mask = calcul_masque_atomes_definis(targets)\n",
    "    \n",
    "    \n",
    "    # On masque les prédictions et les étiquettes selon le masque des atomes définis\n",
    "    targets_masked = tf.boolean_mask(targets, defined_atoms_mask)\n",
    "    predictions_masked = tf.boolean_mask(predictions, defined_atoms_mask)\n",
    "    \n",
    "    return tf.sqrt(tf.reduce_mean(tf.squared_difference(predictions_masked, targets_masked)), name=\"rmse\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonction d'évaluation des performances (score R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_r2_score(predictions, targets, inputs):\n",
    "    \"\"\" Renvoie le score R2 de la prédiction (le calcul est effectué uniquement sur les résultats\n",
    "    des atomes donnés en entrée) \"\"\"\n",
    "    \n",
    "    # On met les prédictions et les cibles sous la forme d'une matrice (200, 4)\n",
    "    predictions = tf.reshape(predictions, [-1, 4])\n",
    "    targets = tf.reshape(targets, [-1, 4])\n",
    "    \n",
    "    # On calcule le masque des atomes définis selon les cibles\n",
    "    defined_atoms_mask = calcul_masque_atomes_definis(targets)\n",
    "    \n",
    "    # On masque les prédictions et les étiquettes selon le masque des atomes définis\n",
    "    targets_masked = tf.boolean_mask(targets, defined_atoms_mask)\n",
    "    predictions_masked = tf.boolean_mask(predictions, defined_atoms_mask)\n",
    "    \n",
    "    # Calcul de l'erreur totale\n",
    "    total_error = tf.reduce_sum(tf.square(tf.subtract(targets, tf.reduce_mean(targets_masked))))\n",
    "                \n",
    "    # Calcul de l'erreur inexpliquée\n",
    "    unexplained_error = tf.reduce_sum(tf.square(tf.subtract(targets_masked, predictions_masked)))\n",
    "    \n",
    "    r2 = tf.subtract(1.0, tf.divide(unexplained_error, total_error), \"r2_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Définition d'une fonction créant le RN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/etudiant/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.data_preprocessing import DataPreprocessing\n",
    "\n",
    "def preprocess_fun(array):\n",
    "    # print(array)\n",
    "    return array.tolist()\n",
    "\n",
    "def creer_fc3_1k_4x15k_800(do=0.99, stddev_init=0.001, learning_rate=0.05):\n",
    "    \"\"\" Fonction créant un réseau de neurones de type fully connected, ayant une couche d'entrée de 1000\n",
    "    neurones, quatre couches cachées de 15000 neurones et une sortie de 800 neurones (en plus d'une couche \n",
    "    d'un neurone de calcul du coût)\n",
    "    Inputs \n",
    "    dropout : proba que chaque neurone ne soit pas désactivé\n",
    "    stddev_init : sigma de l'initialisation des poids\n",
    "    learning_rate : taux d'apprentissage\n",
    "    \"\"\"\n",
    "\n",
    "    # On créé l'initialisateur de tenseur avec une loi normale tronquée. sigma = stddev_init, et les \n",
    "    # valeurs à plus de 2sigma sont re-tirées\n",
    "    winit = tfl.initializations.truncated_normal(stddev=stddev_init, dtype=tf.float32, seed=None)\n",
    "\n",
    "    preprocess = DataPreprocessing(name=\"preprocessor\")\n",
    "    preprocess.add_custom_preprocessing(preprocess_fun)\n",
    "    \n",
    "    # On créé l'input du RN\n",
    "    network = input_data(shape=[None, 1000], name='input', data_preprocessing=preprocess)\n",
    "\n",
    "    \"\"\"   \n",
    "    # On créé les quatre couches cachées\n",
    "    for i in range(1):\n",
    "        network = fully_connected(network, 15000, activation='relu', name='fc'+str(i), weights_init=winit)\n",
    "        \n",
    "    # On détruit des neurones aléatoirement avec une la probabilité donnée en entrée\n",
    "    network = dropout(network, do)\n",
    "    \n",
    "\n",
    "    # On ajoute la couche de sortie du réseau\n",
    "    # Fonction d'activation prelu\n",
    "    # Initilisée avec la loi normale tronquée\"\"\"\n",
    "    network = fully_connected(network, 800, activation='prelu', name='outlayer', weights_init=winit)\n",
    "    # Couche d'évaluation du modèle. Utilisation d'une descente stochastique Adam\n",
    "    # Learning rate = 0.05\n",
    "    # Loss = fonction définie rmse\n",
    "    network = regression(network, optimizer='adam', learning_rate=0.05,\n",
    "        loss=partial_rmse, metric=partial_r2_score, name='target')\n",
    "    \n",
    "    tf.trainable_variables()\n",
    "    \n",
    "    print(\"Fin création RN\")\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation des données\n",
    "\n",
    "#### Fonction renvoyant deux sous-ensembles du jeu d'entrainement : un ensemble d'exemples et les cibles correspondantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold(train_set, targets, reduce_train_fold_proportion):\n",
    "    \"\"\" Permet d'obtenir deux sous-ensembles du jeu d'entraînement afin de ne pas travailler sur le jeu\n",
    "    d'entraînement total pour la recherche par quadrillage et donc de gagner du temps d'exécution. L'idée\n",
    "    et que si un ensemble d'hyper-paramètres produit des meilleurs résultats que les autres ensembles\n",
    "    d'hyper-paramètres sur l'ensemble du jeu d'entraînement, alors on suppose que ce sera également \n",
    "    le cas sur une partie des données. \"\"\"\n",
    "            \n",
    "    size = int(len(train_set[\"inputs\"]) * reduce_train_fold_proportion)\n",
    "    print(size)\n",
    "    return (np.array(train_set[\"inputs\"][:size].flat), np.array(targets[\"targets\"][:size]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonction d'entraînement de modèles pour recherche des meilleurs hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import tflearn as tfl\n",
    "import time\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train(input_path, targets_path, fold_prop):\n",
    "    \n",
    "    inputs_h5 = h5py.File(input_path, 'r')\n",
    "    targets_h5 = h5py.File(targets_path, 'r')\n",
    "    \n",
    "    dropout_vals = [0.99, 0.95]\n",
    "    stddev_init_vals = [0.001, 0.003]\n",
    "    learning_rate_vals = [0.05, 0.1]\n",
    "    \n",
    "    input_X, labels_y = get_fold(inputs_h5, targets_h5, fold_prop)\n",
    "    \n",
    "    input_X = inputs_h5[\"inputs\"][:5]\n",
    "    targets_y = targets_h5[\"targets\"][:5] \n",
    " \n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    # On itère sur toutes les combinaisons d'hyperparamètres\n",
    "    for dropout in dropout_vals:\n",
    "        for stddev_init in stddev_init_vals:\n",
    "            for learning_rate in learning_rate_vals:\n",
    "                \n",
    "                start_time = time.time()\n",
    "                \n",
    "                nom_modele = \"DELTA_DIST+H_01_hyperparam_do-\"+str(dropout)+\"_stdinit_\"+str(stddev_init)+\"_lr_\"+str(learning_rate)\n",
    "                \n",
    "                print(\"Training model \"+nom_modele)\n",
    "                \n",
    "                \n",
    "                # On créé le réseau avec les hyperparamètres courants\n",
    "                network = creer_fc3_1k_4x15k_800(do=dropout, learning_rate=learning_rate, \n",
    "                                                 stddev_init=stddev_init)\n",
    "                \n",
    "                # On créé le modèle\n",
    "                model = tfl.DNN(network, tensorboard_verbose=3,\n",
    "                tensorboard_dir='./tflearn_logs/')\n",
    "            \n",
    "                # Entraînement\n",
    "                model.fit(X_inputs=input_X,Y_targets=labels_y, batch_size=100,\n",
    "                          shuffle = True, snapshot_step=100, validation_set=0.1,\n",
    "                          show_metric=True, run_id=\"hyperparametres_\"+nom_modele , n_epoch=5)\n",
    "\n",
    "                model.save(\"./models/test_\" + nom_modele)\n",
    "                \n",
    "                print(\"Temps d'entraînement du modèle : \")\n",
    "                print(\"--- %s seconds ---\" % (time.time() - total_start_time))\n",
    "\n",
    "    \n",
    "    \n",
    "    print(\"Temps total :\")\n",
    "    print(\"--- %s seconds ---\" % (time.time() - total_start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "Training model DELTA_DIST+H_01_hyperparam_do-0.99_stdinit_0.001_lr_0.05\n",
      "Fin création RN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/etudiant/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Run id: hyperparametres_DELTA_DIST+H_01_hyperparam_do-0.99_stdinit_0.001_lr_0.05\n",
      "Log directory: ./tflearn_logs/\n",
      "---------------------------------\n",
      "Training samples: 4\n",
      "Validation samples: 1\n",
      "--\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3f9535e91102>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminimal_prepared_input_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminimal_labels_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-d5ba9a40bc27>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_path, targets_path, fold_prop)\u001b[0m\n\u001b[1;32m     45\u001b[0m                 model.fit(X_inputs=input_X,Y_targets=labels_y, batch_size=100,\n\u001b[1;32m     46\u001b[0m                           \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnapshot_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                           show_metric=True, run_id=\"hyperparametres_\"+nom_modele , n_epoch=5)\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./models/test_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnom_modele\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tflearn/models/dnn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_inputs, Y_targets, n_epoch, validation_set, show_metric, batch_size, shuffle, snapshot_epoch, snapshot_step, excl_trainops, validation_batch_size, run_id, callbacks)\u001b[0m\n\u001b[1;32m    204\u001b[0m                          \u001b[0mexcl_trainops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexcl_trainops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                          \u001b[0mrun_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                          callbacks=callbacks)\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mretrieve_data_preprocessing_and_augmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tflearn/helpers/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, feed_dicts, n_epoch, val_feed_dicts, show_metric, snapshot_step, snapshot_epoch, shuffle_all, dprep_dict, daug_dict, excl_trainops, run_id, callbacks)\u001b[0m\n\u001b[1;32m    336\u001b[0m                                                        \u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0msnapshot_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m                                                        \u001b[0msnapshot_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                                                        show_metric)\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m                             \u001b[0;31m# Update training state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tflearn/helpers/trainer.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, training_step, snapshot_epoch, snapshot_step, show_metric)\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0mtflearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         _, train_summ_str = self.session.run([self.train, self.summ_op],\n\u001b[0;32m--> 817\u001b[0;31m                                              feed_batch)\n\u001b[0m\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0;31m# Retrieve loss value from summary string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "train(minimal_prepared_input_loc, minimal_labels_loc, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tflearn.datasets.mnist as mnist\n",
    "X, Y, testX, testY = mnist.load_data(one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    " print(np.array(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(X[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    " x = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
